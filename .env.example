# Default STT adapter to use (local, local_gpu, or openai)
# If not set, defaults to "local"
# Options:
#   local     - CPU-based Faster Whisper (int8 quantization)
#   local_gpu - GPU-based Faster Whisper (int8/float16, requires CUDA)
#   openai    - OpenAI Whisper API (requires API key)
PADC_ADAPTER=local

# For GPU support, you may need to set the library path
# Uncomment and adjust the path based on your Python version if needed:
# export LD_LIBRARY_PATH=/path/to/venv/lib/python3.12/site-packages/nvidia/cudnn/lib:$LD_LIBRARY_PATH

# Whisper model size for local/local-gpu adapters
# Options: tiny, base, small, medium, large, large-v2, large-v3
# Larger models are more accurate but slower
WHISPER_MODEL=base

# Audio buffer size in seconds (how much audio to keep in memory)
# Default: 30.0 seconds
# Increase for longer dictation sessions, decrease to save memory
PADC_BUFFER_SECONDS=30.0

# Audio gain normalization target level (0.0 to 1.0)
# Default: 0.7 (70% of maximum, leaves headroom to prevent clipping)
# Set to 0.0 to disable normalization and use raw microphone levels
# Higher values = louder but more risk of clipping; lower = quieter
PADC_NORMALIZE_AUDIO=0.7

# Debug: Save processed audio buffers to WAV files (for troubleshooting)
# When enabled, saves each audio buffer to debug_audio/ before transcription
# Set to "true" to enable, any other value or unset to disable
# WARNING: Files can accumulate quickly, remember to clean debug_audio/ periodically
PADC_DEBUG_SAVE_AUDIO=false

# Real-time mode configuration
# Real-time mode automatically transcribes based on silence detection or max interval
# Enable with: echo "toggle-realtime" > /tmp/padc.fifo

# Seconds of silence before auto-transcription (default: 5.0)
PADC_REALTIME_SILENCE_THRESHOLD=5.0

# Maximum seconds between transcriptions (if speaking continuously) (default: 20.0)
PADC_REALTIME_MAX_INTERVAL=20.0

# RMS level below which audio is considered silence (default: 0.01)
# Lower = more sensitive to quiet sounds, higher = requires louder audio
PADC_SILENCE_RMS_THRESHOLD=0.01

# Silence gap filtering threshold in seconds
# When a silence gap longer than this threshold is detected between speech segments,
# all segments before the gap are discarded (considered "stale" speech)
# This prevents old/irrelevant speech from being included in transcriptions
# Default: 20.0 seconds
# Set higher to be more conservative (keep more old speech)
# Set lower to be more aggressive (discard speech after shorter pauses)
# Set to 0 to disable filtering and keep all segments regardless of silence gaps
PADC_SILENCE_CUTOFF_SECONDS=20.0

# Language code for transcription (ISO 639-1 format)
# Default: en (English)
# Common options: en, fr, es, de, it, pt, ja, zh, etc.
# This applies to GPU transcription (Faster Whisper)
# Note: OpenAI commands (*-fr) use their own language setting
PADC_LANGUAGE=en

# OpenAI API configuration for French transcription (or any non-English language)
# The daemon now supports dual transcription engines:
#   - GPU (Faster Whisper): Fast, local, configurable language - used by default commands
#   - OpenAI API: Cloud-based, multi-language - used by *-fr commands
# Copy this file to .env and add your actual API key
OPENAI_API_KEY=your_openai_api_key_here

# OpenAI Whisper model to use (default: whisper-1)
# Available models: whisper-1 (only option as of now)
PADC_OPENAI_MODEL=whisper-1