# Default STT adapter to use (local, local_gpu, or openai)
# If not set, defaults to "local"
# Options:
#   local     - CPU-based Faster Whisper (int8 quantization)
#   local_gpu - GPU-based Faster Whisper (int8/float16, requires CUDA)
#   openai    - OpenAI Whisper API (requires API key)
PADC_ADAPTER=local

# For GPU support, you may need to set the library path
# Uncomment and adjust the path based on your Python version if needed:
# export LD_LIBRARY_PATH=/path/to/venv/lib/python3.12/site-packages/nvidia/cudnn/lib:$LD_LIBRARY_PATH

# Whisper model size for local/local-gpu adapters
# Options: tiny, base, small, medium, large, large-v2, large-v3
# Larger models are more accurate but slower
WHISPER_MODEL=base

# Audio buffer size in seconds (how much audio to keep in memory)
# Default: 30.0 seconds
# Increase for longer dictation sessions, decrease to save memory
PADC_BUFFER_SECONDS=30.0

# Audio gain normalization target level (0.0 to 1.0)
# Default: 0.7 (70% of maximum, leaves headroom to prevent clipping)
# Set to 0.0 to disable normalization and use raw microphone levels
# Higher values = louder but more risk of clipping; lower = quieter
PADC_NORMALIZE_AUDIO=0.7

# Debug: Save processed audio buffers to WAV files (for troubleshooting)
# When enabled, saves each audio buffer to debug_audio/ before transcription
# Set to "true" to enable, any other value or unset to disable
# WARNING: Files can accumulate quickly, remember to clean debug_audio/ periodically
PADC_DEBUG_SAVE_AUDIO=false

# OpenAI API key for using the OpenAI adapter
# Copy this file to .env and add your actual API key
OPENAI_API_KEY=your_openai_api_key_here